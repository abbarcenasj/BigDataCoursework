{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. (100 points)\n",
    "\n",
    "In this exercise you will use Spark to build and run a machine learning pipeline to separate 'ham' from 'spam' in SMS text messages. Then you will use the pipeline to classify SMS texts.\n",
    "\n",
    "- Create a Pandas DataFraem form the data in the file`SMSSpamCollection` where each line is tab separated into (label, text). If you find that the read_xxx function in Pandas does not do the job correctly, read in the file line by line before converting to a DataFrame. Create an index column so that each row has a unique number id.\n",
    "- Convert to a Spark DataFrame that has two columns (klass, SMS) and split into test and training data sets with proportions 0.8 and 0.2 respectively using a random seed of 123.\n",
    "- Build a Spark ML pipeline consisting of the following \n",
    "    - StringIndexer: To convert `klass` into a numeric `labels` column\n",
    "    - Tokenizer: To covert `SMS` into a list of tokens\n",
    "    - StopWordsRemover: To remove \"stop words\" from the tokens\n",
    "    - CountVectorizer: To count words (use a vocabular size of 100 and minimum number of occureences of 2)\n",
    "    - LogisticRegression: Use `maxIter=10`, `regParam=0.001`\n",
    "\n",
    "- Train the model on the test data.\n",
    "- Evaluate the precision, recall and accuracy of this model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_0 = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and create a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/cliburn/bios-823-2019/master/homework/SMSSpamCollection\"\n",
    "data = pd.read_csv(url,\"\\t\", header=None, names=[\"label\",\"text\"])\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[klass: string, SMS: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['klass', 'SMS']\n",
    "df = spark_0.createDataFrame(data[['label', 'text']], cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the code in train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[klass: string, SMS: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=123)\n",
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|klass|                 SMS|\n",
      "+-----+--------------------+\n",
      "|  ham| said kiss, kiss,...|\n",
      "|  ham|4 oclock at mine....|\n",
      "|  ham|7 at esplanade.. ...|\n",
      "|  ham|8 at the latest, ...|\n",
      "|  ham|A famous quote : ...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "    inputCol=\"klass\", \n",
    "    outputCol=\"label\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    inputCol=\"SMS\",\n",
    "    outputCol=\"tokens\"\n",
    ")\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"tokens\",\n",
    "    outputCol=\"filtered\"\n",
    ")\n",
    "\n",
    "CountVectorizerModel = CountVectorizer(\n",
    "    inputCol=\"filtered\",\n",
    "    outputCol=\"features\",\n",
    "    vocabSize=100,\n",
    "    minDF=2\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    regParam=0.001\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, remover, CountVectorizerModel,lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = prediction.select(['label', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = float(score.rdd.map(lambda x: x[0]==1 and x[1]==1).sum())\n",
    "tn = float(score.rdd.map(lambda x: x[0]==0 and x[1]==0).sum())\n",
    "fp = float(score.rdd.map(lambda x: x[0]==0 and x[1]==1).sum())\n",
    "fn = float(score.rdd.map(lambda x: x[0]==1 and x[1]==0).sum())\n",
    "p = float(score.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9617058311575283\n",
      "Recall = 0.7972972972972973\n",
      "Precision = 0.8939393939393939\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = %s' % ((tp+tn)/p))\n",
    "print('Recall = %s' % (tp/(tp+fn)))\n",
    "print('Precision = %s' % (tp/(tp+fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** (100 points)\n",
    "\n",
    "In this exercise, you will simulate running a machine learning pipeline to classify steaming data.\n",
    "\n",
    "- Convert the test DataFrame into a Pandas DataFrame\n",
    "- Write each row of the DataFrame to a separate tab-delimited file in a folder called \"incoming_sms\"\n",
    "- Create a Structured Streaming DataFrame using `readStream` with `option(\"maxFilesPerTrigger\", 1)` to simulate streaming data\n",
    "- Use the fitted pipeline created in Ex. 1 to transform the input stream\n",
    "- Write the transformed stream to memory with name `sms_pred\n",
    "- Sleep 30 seconds\n",
    "- Use an SQL query to show the `index`, `label` and `prediction` columns\n",
    "- Sleep 30 more seconds\n",
    "- Use an SQL query to show the `index`, `label` and `prediction` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "spark_1 = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local\") \n",
    "    .appName(\"BIOS-823\") \n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    .getOrCreate()    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converto to pandas dataframe and create folder \"incoming_sms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test.toPandas()\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dir_path = 'incoming_sms/'\n",
    "if os.path.exists(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "os.makedirs(dir_path)\n",
    "\n",
    "for i in range(test_df.shape[0]):\n",
    "    filename = 'file{}.csv'.format(i)\n",
    "    full_name = dir_path+filename\n",
    "    test_df.iloc[[i]].to_csv(full_name ,sep = \"\\t\", encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = test.schema\n",
    "dir_path = 'incoming_sms/'\n",
    "\n",
    "streamingDF = (\n",
    "  spark_1\n",
    "    .readStream\n",
    "    .schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(dir_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pipeline and save to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_pipeline = (\n",
    "    model\n",
    "    .transform(streamingDF)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write query stream to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "query = (\n",
    "    stream_pipeline.writeStream.\n",
    "    queryName(\"sms_pred\").\n",
    "    format(\"memory\").\n",
    "    start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    sleep(30)\n",
    "    spark_1.sql('''\n",
    "    SELECT label, prediction \n",
    "    FROM sms_pred\n",
    "    ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
